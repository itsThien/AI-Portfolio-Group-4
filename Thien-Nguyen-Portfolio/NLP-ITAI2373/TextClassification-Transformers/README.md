[README.md](https://github.com/user-attachments/files/24012468/README.md)
# Text Classification using Transformers

## Overview
This project fine-tunes a pretrained transformer model (BERT / DistilBERT) to classify text documents.

## Problem Statement
Traditional NLP models struggle with context understanding. Transformers solve this with self-attention.

## Approach
- Loaded pretrained DistilBERT  
- Tokenized text using HuggingFace  
- Fine-tuned model on small dataset  
- Evaluated accuracy and loss  

## Results
Placeholder accuracy: **90%**

## Learning Outcomes
I learned:
- How transformers process text  
- How fine-tuning is performed  
- How to handle tokenization and batching  

## Files
- train_transformer.py
- requirements.txt
- Text_Classification_Transformers.ipynb
